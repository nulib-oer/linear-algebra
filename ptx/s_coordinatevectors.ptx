<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_coordinatevectors" >
  <title>Coordinate vectors</title>
  <introduction>
    <p>
    Suppose <m>V</m> is an <m>n</m>-dimensional vector space. Once we choose a basis <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> of <m>V</m>, we know from <xref ref="th_basis_equivalence"/> that any <m>\boldv\in V</m> can be expressed <em>in a unique way</em> as
     <me>
       \boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n, c_i\in \R
     </me>.
     <em>Coordinate vectors</em> turn this observation into a computational tool by exploiting the resulting one-to-one correspondence
     <men xml:id="eq_coordinate_correspondence">
       \boldv\in V \longleftrightarrow (c_1,c_2,\dots, c_n)\in \R^n
     </men>.
      We will use the correspondence <xref ref="eq_coordinate_correspondence"/> in two distinct ways, as described below.
      <ol>
        <li>
          <p>
            Given an <m>n</m>-dimensional vector space <m>V</m> and basis <m>B</m>, the correspondence <xref ref="eq_coordinate_correspondence"/> allow us to treat elements of the abstract space <m>V</m> as if they were elements of <m>\R^n</m>, and to then make use of our wealth of computational procedures related to <m>n</m>-tuples.
          </p>
        </li>
        <li>
          <p>
            The correspondence <xref ref="eq_coordinate_correspondence"/> is also useful when working in <m>\R^n</m> itself. Namely, there will be situations where it is convenient to represent vectors with a particular nonstandard basis <m>B</m>, as opposed to the standard basis <m>\{\bolde_1, \bolde_2, \dots, \bolde_n\}</m>. In this setting the correspondence <xref ref="eq_coordinate_correspondence"/> will be used as a <q>change of coordinates</q> technique.
          </p>
        </li>
      </ol>
</p>

  </introduction>
  <subsection xml:id="ss_coordinate_vectors">
    <title>Coordinate vectors</title>
    <p>
      Before we can define coordinate vectors we need to define an <em>ordered basis</em>. As the name suggests this is nothing more than a basis along with a particular choice of ordering of its elements: <ie /> first element, second element, <etc />. In other words, an ordered basis will be a <em>sequence</em> of vectors, as opposed to a  <em>set</em> of vectors.
    </p>
    <definition xml:id="d_ordered_basis">
      <title>Ordered bases</title>
      <idx><h>ordered basis</h></idx>
      <statement>
        <p>
        Let <m>V</m> be a finite-dimensional vector space. An <term>ordered basis</term> of <m>V</m> is a sequence of distinct vectors <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> whose underlying set <m>\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> is a basis of <m>V</m>.
        </p>
      </statement>
    </definition>
      <remark xml:id="rm_orthogonal_ordered_basis">
      <statement>
        <p>
          If <m>V</m> is further given the structure of a (finite-dimensional) inner product space, then we say an ordered basis <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> is orthogonal (resp., orthonormal) if its underlying set is orthogonal (resp. orthonormal).
        </p>
      </statement>
    </remark>

        <remark >
      <statement>
        <p>
        A single (unordered) basis <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> of an <m>n</m>-dimensional vector space gives rise to <m>n!</m> different ordered bases: you have <m>n</m> choices for the first element of the ordered basis, <m>(n-1)</m> choices for the second element, <etc />.
        </p>
        <p>
          For example the standard basis <m>B=\{\bolde_1, \bolde_2, \bolde_3\}</m> of <m>\R^3</m> gives rise to <m>3!=6</m> different ordered bases of <m>\R^3</m>:
          <md>
            <mrow>B_1\amp =(\bolde_1, \bolde_2, \bolde_3) \amp B_2\amp =(\bolde_1, \bolde_3, \bolde_2) </mrow>
            <mrow> B_3\amp=(\bolde_2, \bolde_1, \bolde_3) \amp B_4\amp =(\bolde_2, \bolde_3, \bolde_1) </mrow>
            <mrow> B_5 \amp =(\bolde_3, \bolde_1, \bolde_2) \amp B_6\amp =(\bolde_3, \bolde_2, \bolde_1)</mrow>
          </md>.
        </p>
        <p>
        By a slight abuse of language we will use <q>standard basis</q> to describe both one of our standard unordered bases and the corresponding ordered basis obtained by choosing the implicit ordering of the set descriptions in <xref ref="rm_standard_bases"/>. For example, <m>\{x^2, x, 1\}</m> and <m>(x^2, x, 1)</m> will both be called the standard basis of <m>P_2</m>.
        </p>
      </statement>
    </remark>

    <definition xml:id="d_coordinatevector">
      <title>Coordinate vectors</title>
      <statement>
        <p>
          Let <m>B=(\boldv_1, \boldv_2,\dots , \boldv_n)</m> be an ordered basis for the vector space <m>V</m>. According to <xref ref="th_basis_equivalence"/>, for any <m>\boldv\in V</m> there is a <em>unique</em> choice of scalars <m>c_i\in \R</m> satisfying
          <me>
            \boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_n\boldv_n
          </me>.
          We call the corresponding <m>n</m>-tuple <m>(c_1,c_2,\dots, c_n)</m> the <term>coordinate vector of <m>\boldv</m> relative to the basis <m>B</m></term>, and denote it <m>[\boldv]_B</m>: <ie />,
          <me>
            [\boldv]_B=(c_1,c_2,\dots, c_n)
          </me>.
        </p>
      </statement>
    </definition>
          <p>
            To compute the coordinate vector of an element <m>\boldv\in V</m> relative to a given ordered basis
            <m>B=(\boldv_1,\boldv_2,\dots, \boldv_n)</m> we must <em>find</em> the scalars <m>c_1, c_2, \dots, c_n</m>
            that satisfy the vector equation
            <me>
            \boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n
            </me>.
            Usually this is accomplished by reducing this vector equation to an equivalent system of linear equations in the unknowns <m>c_i</m> and solving using the method of Gaussian elimination. However, there are some cases when we can easily produce the <m>c_i</m> by inspection: for example
            when computing with standard bases as the first example below illustrates.
          </p>
          <p>Furthermore, if the given basis happens to be orthogonal (or orthonormal) with respect to some inner product, <xref ref="th_coordinates_orthogonal"/> provides an inner product formula for computing the <m>c_i</m>.
          </p>
  <example xml:id="eg_coordinatevector_standard">
    <title>Standard bases</title>
    <statement>
      <p>
        Computing coordinate vectors relative to one of our standard bases for <m>\R^n</m>, <m>M_{mn}</m>, or <m>P_{n}</m> amounts to just listing the coefficients or entries used to specify the given vector. The examples below serve to illustrate the general method in this setting.
      </p>
      <ol>
        <li>
          <p>
          Let <m>V=\R^3</m> and <m>B=(\bolde_1, \bolde_2, \bolde_3)</m>. For any <m>\boldv=(a,b,c)\in \R^3</m> we have <m>[\boldv]_{B}=(a,b,c)</m>, since <m>(a,b,c)=a\bolde_1+b\bolde_2+c\bolde_3</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>V=M_{22}</m> and <m>B=(E_{11}, E_{12}, E_{21}, E_{22})</m>. For any <m>A=\begin{amatrix}[rr]a\amp b\\ c\amp d \end{amatrix}</m> we have <m>[A]_B=(a,b,c,d)</m> since
            <me>
              A=aE_{11}+bE_{12}+cE_{21}+dE_{22}
            </me>.
          </p>
        </li>
      </ol>
    </statement>

  </example>
  <example>
    <title>Reorderings of standard bases</title>
    <statement>
      <p>
      If we choose an alternate ordering of one of the standard bases, the entries of the coordinate vector are reordered accordingly, as illustrated by the examples below.
      </p>
      <ol>
        <li>
          <p>
            Let <m>V=\R^3</m> and <m>B=(\bolde_2, \bolde_1, \bolde_3)</m>. Given <m>\boldv=(a,b,c)\in \R^3</m> we have <m>[\boldv]_B=(b,a,c)</m>, since
            <me>
            \boldv=b\bolde_2+a\bolde_1+c\bolde_3
            </me>.
          </p>
        </li>
        <li>
          <p>
            Let <m>P=P_3</m> and <m>B=(1,x,x^2, x^3)</m>. Given <m>p(x)=ax^3+bx^2+cx+d</m> we have <m>[p(x)]_B=(d, c, b, a)</m>, since
            <me>
              p(x)=(d)1+cx+bx^2+ax^3
            </me>.
          </p>
        </li>
      </ol>
    </statement>
  </example>
  <example>
    <title>Nonstandard bases</title>

    <statement>
      <p>
        For a nonstandard ordered basis, we compute coordinate vectors by solving a relevant system of linear equations, as the examples below illustrate.
      </p>
      <ol>
        <li>
          <p>
            Let <m>V=\R^2</m>, <m>B=((1,2),(1,1))</m>, and <m>\boldv=(3,3)</m>. Compute <m>[\boldv]_B</m>. More generally, compute <m>[(a,b)]_B</m> for an arbitrary <m>(a,b)\in \R^2</m>.
          </p>
        </li>
        <li>
          <p>
          Let <m>V=P_2</m>, <m>B=(x^2+x+1, x^2-x, x^2-1)</m>, and <m>p(x)=x^2</m>. Compute <m>[p(x)]_B</m>. More generally, compute <m>[ax^2+bx+c]</m> for an arbitrary element <m>ax^2+bx+c\in P_2</m>.
          </p>
        </li>
      </ol>
    </statement>
    <solution>
      <ol>
        <li>
          <p>
           To compute <m>[(3,3)]_B</m> we must find the unique pair <m>(c_1, c_2)</m> satisfying
            <me>
              (3,3)=c_1(1,2)+c_2(1,1)
            </me>.
            By inspection, we see that
            <me>
              (3,3)=3(1,1)=0(1,2)+3(1,1)
            </me>.
            We conclude that
            <me>
              [\boldv]_{B}=(0,3)
            </me>.
            More generally, to compute <m>[\boldv]_B</m> for an arbitrary <m>\boldv=(a,b)\in \R^2</m>, we must find the pair <m>(c_1,c_2)</m> satisfying <m>(a,b)=c_1(1,2)+c_2(1,1)</m>, or equivalently
            <me>
              \begin{linsys}{2}
                c_1\amp +\amp c_2 \amp =\amp a\\
                2c_1\amp +\amp c_2\amp =\amp b
              \end{linsys}
            </me>.
             The usual techniques yield the unique solution <m>(c_1,c_2)=(-a+b,2a-b)</m>, and thus
             <me>
               [\boldv]_B=(-a+b, 2a-b)
             </me>
             for <m>\boldv=(a,b)</m>.
          </p>
        </li>
        <li>
          <p>
           To compute <m>[x^2]_B</m> we must find the unique triple <m>(c_1,c_2,c_3)</m> satisfying
          <me>
            x^2=c_1(x^2+x+1)+c_2(x^2-x)+c_3(x^2-1)
          </me>.
          The equivalent linear system once we combine like terms and equate coefficients is
          <me>
            \begin{linsys}{3}
              c_1\amp +\amp c_2\amp +\amp c_3\amp =\amp 1\\
              c_1\amp -\amp c_2\amp \amp \amp =\amp 0\\
              c_1\amp \amp \amp -\amp c_3\amp =\amp 0\\
            \end{linsys}
          </me>.
          The unique solution to this system is <m>(c_1,c_2,c_3)=(1/3, 1/3, 1/3)</m>. We conclude
          <me>
            [x^2]_B=\frac{1}{3}(1, 1, 1)
          </me>.
          The same reasoning shows that more generally, given polynomial <m>p(x)=ax^2+bx+c</m>, we have
          <me>
            [p(x)]_B=\frac{1}{3}(a+b+c, a-2b+c, a+b-2)
          </me>.
          </p>
        </li>
      </ol>
    </solution>
  </example>
  <p>
    When <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> is an orthogonal basis with respect to some inner product <m>\langle , \rangle </m>, then by <xref ref="th_orthogonal_basis_formula"/> we have
    <me>
      \boldv=\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }\boldv_1+\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }\boldv_1+\cdots +\frac{\langle \boldv,\boldv_n  \rangle }{\langle \boldv_n, \boldv_n\rangle }\boldv_n
    </me>,
    and thus
    <me>
      [\boldv]_B=\left(\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }, \frac{\langle \boldv,\boldv_2  \rangle }{\langle \boldv_2, \boldv_2\rangle },\dots, \frac{\langle \boldv,\boldv_n  \rangle }{\langle \boldv_n, \boldv_n\rangle }\right)
    </me>.
    We have thus proved the following theorem.
  </p>
  <theorem xml:id="th_coordinates_orthogonal">
    <title>Coordinate vectors for orthogonal bases</title>
    <statement>
      <p>
      Let <m>(V,\langle , \rangle )</m> be an inner product space, and suppose the ordered basis <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> is orthogonal with respect to <m>\langle , \rangle </m>. For all <m>\boldv\in V</m> we have
      <me>
        [\boldv]_B=\left(\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }, \frac{\langle \boldv,\boldv_2  \rangle }{\langle \boldv_2, \boldv_2\rangle },\dots, \frac{\langle \boldv,\boldv_n  \rangle }{\langle \boldv_n, \boldv_n\rangle }\right)
      </me>.
      </p>
    </statement>
  </theorem>
  <example>
    <title>Orthogonal bases</title>
    <statement>
      <p>
        Let <m>V=\R^2</m> and <m>B=((1,1),(-1,2))</m>. Find a general formula for <m>[(a,b)]_B</m>. Note: <m>B</m> is orthogonal with respect to the weighted dot product
        <me>
          \langle (x_1,x_2), (y_1,y_2)\rangle =2x_1y_1+x_2y_2
        </me>.
      </p>
    </statement>
    <solution>
      <p>
        Applying <xref ref="th_coordinates_orthogonal"/> to <m>B</m> and the dot product with weights <m>2, 1</m>, for any <m>\boldv=(a,b)</m> we compute
        <md>
          <mrow>[(a,b)]_B \amp =\left(\frac{\langle (a,b), (1,1)\rangle }{\langle (1,1),(1,1) \rangle }, \frac{\langle (a,b), (-1,2)\rangle }{\langle (-1,2),(-1,2) \rangle }\right)</mrow>
          <mrow> \amp=\left(\frac{1}{3}(2a+b),\frac{1}{3}(-a+b) \right) </mrow>
        </md>.
        Let's check our formula with <m>\boldv=(3,-3)</m>. The formula yields <m>[(3,-3)]_B=(1,-2)</m>, and indeed we see that
        <me>
          (3,-3)=1(1,1)-2(-1,2)
        </me>.

      </p>
    </solution>
  </example>
</subsection>
<subsection xml:id="ss_coordinate_transformation">
  <title>Coordinate vector transformation</title>
  <p>
    The next theorem is the key to understanding the tremendous computational value of coordinate vectors. Here we treat the coordinate vector operation as a function
    <md>
      <mrow>[\phantom{v}]_B\colon V\amp \rightarrow \R^n
      </mrow>
      <mrow> \boldv\amp\mapsto [\boldv]_B\in \R^n </mrow>
    </md>.
    Not surprisingly, this turns out to be a linear transformation, which we call a <em>coordinate vector transformation</em>. Furthermore, the correspondence
    <me>
      \boldv\in V \longmapsto [\boldv]_B\in \R^n
    </me>
    is a one-to-one correspondence between <m>V</m> and <m>\R^n</m>, allowing us to identify the vectors <m>\boldv\in V</m> with <m>n</m>-tuples in <m>\R^n</m>. In the language of <xref ref="s_isom"/>, these two facts taken together mean that the coordinate vector transformation is an <xref ref="d_isomorphism" text="custom">isomorphism</xref> between <m>V</m> and <m>\R^n</m>. Practically speaking, this means any question regarding the vector space structure of <m>V</m> can be translated to an equivalent question about the vector space <m>\R^n</m>. As a result, given any <q>exotic</q> vector space <m>V</m> of finite dimension, once we choose an ordered basis <m>B</m> of <m>V</m>, questions about <m>V</m> can be answered by taking coordinate vectors with respect to <m>B</m>
    and answering the corresponding question in the more familiar setting of <m>\R^n</m>, where we have a wealth of computational procedures at our disposal. We memorialize this principle as a mantra.
  </p>
  <principle xml:id="mantra_coordinate_vectors">
    <title>Coordinate vector mantra</title>
    <statement>
      <p>
        Coordinate vectors allow us to treat any <m>n</m>-dimensional vector space as if it were <m>\R^n</m>.
      </p>
    </statement>
  </principle>
    <theorem xml:id="th_coordinates">
      <title>Coordinate vector transformation</title>
      <statement>
        <p>
          Let <m>B=(\boldv_1,\boldv_2,\dots, \boldv_n)</m> be an ordered basis for the vector space <m>V</m>.
          <ol>
            <li>
              <p>
                The function <m>T\colon V\rightarrow \R^n</m> defined as <m>T(\boldv)=[\boldv]_B</m> is a linear transformation.
                We call <m>T</m> the <term>coordinate vector transformation</term> with respect to <m>B</m>.
              </p>
            </li>
            <li>
              <p>We have
                <m>[\boldv]_B=[\boldw]_B</m> if and only if <m>\boldv=\boldw</m>: <ie />,  <m>T</m> is <xref ref="d_injective_surjective_bijective" text="custom">injective</xref>.
              </p>
            </li>
            <li>
              <p>For all <m>\boldx=(c_1,c_2,\dots, c_n)\in \R^n</m>, there is <m>\boldv\in V</m> with <m>[\boldv]_B=\boldx</m> : <ie />, <m>T</m> is <xref ref="d_injective_surjective_bijective" text="custom">surjective</xref>.
              </p>
            </li>
            <li>
              <p>
                Given a subset <m>S=\{\boldv_1,\boldv_2,\dots,
                \boldv_r\}\subseteq V</m>, let <m>S'=T(S)=\{[\boldv_1]_B,[\boldv_2]_B,\dots,
                [\boldv_r]_B\}\subseteq \R^n</m>. We have the following equivalences:
                <ol>
                  <li>
                    <p>
                      <m>\boldv\in\Span S</m> if and only if <m>[\boldv]_{B}\in \Span S'</m>;
                    </p>
                  </li>
                  <li>
                    <p>
                      <m>S</m> is linearly independent if and only if <m>S'</m> is linearly independent.
                    </p>
                  </li>
                </ol>
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
    <proof>
        <ol>
          <li>
            <p>
              Suppose <m>T(\boldv)=[\boldv]_B=(a_1,a_2,\dots, a_n), T(\boldw)=[\boldw]_B=(b_1, b_2, \dots, b_n)</m>. By definition this means
              <md>
                <mrow>\boldv \amp =\sum_{i=1}^na_i\boldv_i,  \amp \boldw\amp =\sum_{i=1}^nb_i\boldv_i</mrow>
              </md>.
              It follows that
              <me>
                c\boldv+d\boldw=\sum_{i=1}^n(ca_i+db_i)\boldv_i
              </me>,
              and hence
              <md>
                <mrow>T(c\boldv+d\boldw)\amp =[c\boldv+d\boldw]_B \amp (\text{def. of } [\phantom{\boldv}]_B) </mrow>
                <mrow> \amp =(ca_1+db_1,ca_2+db_2,\dots, ca_n+db_n) </mrow>
                <mrow>  \amp =c(a_1,a_2,\dots, a_n)+d(b_1,b_2,\dots, b_n)</mrow>
                <mrow>  \amp =c[\boldv]_B+d[\boldw]_B</mrow>
                <mrow>  \amp =cT(\boldv)+dT(\boldw) </mrow>
              </md>.
              This proves <m>T</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Clearly, if <m>\boldv=\boldw</m>, then <m>[\boldv]_B=[\boldw]_B</m>. If <m>T(\boldv)=T(\boldw)=(c_1,c_2,\dots, c_n)</m>, then by definition of <m>[\phantom{\boldv}]_B</m> we must have
              <me>
                \boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n==\boldw
              </me>.
            </p>
          </li>
          <li>
            <p>
              Given any <m>\boldb=(b_1,b_2,\dots, b_n)\in \R^n</m>, we have <m>\boldb=T(\boldv)</m>, where
              <me>
                \boldv=b_1\boldv_1+b_2\boldv_2+\cdots +b_n\boldv_n
              </me>.
              This proves <m>\im T=\R^n</m>.
            </p>
          </li>
          <li>
            <p>
              We have
              <md>
                <mrow>\boldv\in \Span S \amp \iff \boldv=\sum_{i=1}^rc_i\boldv_i</mrow>
                <mrow> \amp\iff [\boldv]_{B}=\left[ \sum_{i=1}^rc_i\boldv_i\right] </mrow>
                <mrow>  \amp \iff [\boldv]_B=\sum_{i=1}^rc_i[\boldv_i]_B\amp ([\phantom{\boldv}]_B \text{ is linear})</mrow>
                <mrow>  \amp \iff [\boldv]_B\in \Span S'</mrow>
              </md>.
            </p>
            <p>
              Similarly, we have
              <md>
                <mrow>\sum_{i=1}^rc_i\boldv_i=\boldzero \amp \iff \left[\sum_{i=1}^rc_i\boldv_i\right]_B=[\boldzero]_B \amp (\boldv=\boldw\iff [\boldv]_B=[\boldw]_B) </mrow>
                <mrow> \amp\iff \sum_{i=1}^rc_i[\boldv_i]_B=(0,0,\dots, 0) \amp ([\phantom{\boldv}]_B \text{ is linear}) </mrow>
              </md>.
              From this equivalence we see that there is a nontrivial linear combination of <m>S</m> yielding <m>\boldzero\in V</m> if and only if there is a nontrivial linear combination of <m>S'</m> yielding <m>\boldzero\in \R^n</m>. In other words, <m>S</m> is linearly independent if and only if <m>S'</m> is linearly independent.
            </p>
          </li>
        </ol>
    </proof>
        <remark xml:id="rm_coordinate_transform_invertible">
      <statement>
        <p>
          Statements (2) and (3) of <xref ref="th_coordinates"/> tell us that the coordinate transformation is injective (or one-to-one) and surjective (or onto), respectively. (See <xref ref="d_injective_surjective_bijective"/>).
        </p>
      </statement>
    </remark>

  <p>
    As an illustration of the <xref ref="mantra_coordinate_vectors" text="custom">coordinate vector mantra</xref>, we describe a general method of contracting and extending subsets of a general finite-dimensional vector space <m>V</m> to bases. The method translates the problem into <m>\R^n</m> using the coordinate transformation, applies the relevant algorithm available to us for subsets of <m>\R^n</m>, and then <q>lifts</q> the results back to <m>V</m> using the coordinate transformation again.
  </p>
  <algorithm xml:id="proc_contract_extend_general">
    <title>Contracting and extending to bases in general spaces</title>
    <statement>
      <p>
        Let <m>V</m> be a vector space of dimension <m>n</m>, and let
        <m>S=\{\boldv_1, \boldv_2,\dots, \boldv_r\}\subseteq V</m>.
      </p>
      <dl>
        <li>
          <title>Contracting to a basis</title>
          <p>
            Let <m>W=\Span S</m>. To contract <m>S</m> to a basis <m>W</m>, proceed as follows.
          </p>
          <ol>
            <li>
              <p>
                Pick any ordered basis <m>B</m> of <m>V</m> and let <m>S'=\{[\boldv_1]_B, [\boldv_2]_B, \dots, [\boldv_r]_B\}\subseteq \R^n</m>.
              </p>
            </li>
            <li>
              <p>
                Use the relevant procedure of <xref ref="proc_contract_extend"/> to contract <m>S'</m> to a basis <m>T'=\{[\boldv_{i_1}]_B, [\boldv_{i_2}]_B, \dots, [\boldv_{i_k}]_B\}</m> of <m>\Span S'</m>.
              </p>
            </li>
            <li>
              <p>
                The set <m>T=\{\boldv_{i_1}, \boldv_{i_2}, \dots, \boldv_{i_k}\}\subseteq S</m> is a basis for <m>W=\Span S</m>.
              </p>
            </li>
          </ol>
        </li>
        <li>
          <title>Extending to a basis</title>
          <p>
            Assume <m>S</m> is linearly independent. To extend <m>S</m> to a basis of <m>V</m> proceed as follows.
          </p>
          <ol>
            <li>
              <p>
                Pick any ordered basis <m>B</m> of <m>V</m> and let <m>S'=\{[\boldv_1]_B, [\boldv_2]_B, \dots, [\boldv_r]_B\}\subseteq \R^n</m>.
              </p>
            </li>
            <li>
              <p>
                Use the relevant procedure of <xref ref="proc_contract_extend"/> to extend <m>S'</m> to a basis <m>T'=\{[\boldv_1]_B, [\boldv_2]_B, \dots, [\boldv_r]_B, \boldx_{r+1}, \dots, \boldx_n \}</m> of <m>\R^n</m>.
              </p>
            </li>
            <li>
              <p>
                By (3) of <xref ref="th_coordinates"/>, for all <m>r+1\leq i\leq n</m> there are vectors <m>\boldv_{i}\in V</m> satisfying <m>[\boldv_i]_B=\boldx_i</m>. The set <m>T=\{\boldv_1,\dots, \boldv_r,\boldv_{r+1},\dots,\boldv_n\}</m> is a basis of <m>V</m> containing <m>S</m>.
              </p>
            </li>
          </ol>
        </li>
      </dl>
    </statement>
  </algorithm>
  <example>
    <statement>
      <p>
        The set
        <me>
          S=\left \{ A_1=\begin{bmatrix}2\amp 1\\ 0\amp -2 \end{bmatrix} , A_2=\begin{bmatrix}1\amp 1\\ 1\amp -1 \end{bmatrix} , A_3=\begin{bmatrix}0\amp 1\\ 2\amp 0 \end{bmatrix} , A_4=\begin{bmatrix}-1\amp 0\\ 1\amp 1 \end{bmatrix} \right\}
        </me>
        is a subset of the space <m>W=\{ A\in M_{22}\colon \tr A=0\}</m>.
        Let <m>W'=\Span S</m>. Contract <m>S</m> to a basis of <m>W'</m>
        and determine whether <m>W'=W</m>.
      </p>
    </statement>
    <hint>
      <p>
        Choose an ordered basis <m>B</m> of <m>M_{22}</m> and use the coordinate vector map to translate to a question about subspaces of <m>\R^4</m>. Answer this question and translate back to <m>M_{22}</m>.
      </p>
    </hint>
    <solution>
      <p>
        Let <m>B=(E_{11}, E_{12}, E_{21}, E_{22})</m> be the standard basis of <m>M_{22}</m>.
        Apply <m>[\phantom{\boldv}]_B</m> to the elements of the given <m>S</m> to get a corresponding set <m>S'\subseteq\R^4</m>:
        <me>
          S'=\left\{ [A_1]_B=(2,1,0,-2), [A_2]_B=(1,1,1,-1), [A_3]_B=(0,1,2,0), [A_4]_B=(-1,0,1,1) \right\}
        </me>.
        Apply the column space procedure of <xref ref="proc_contract_extend"/> to contract <m>S'</m> to a basis <m>T'</m> of <m>\Span S'</m>. This produces the subset
        <me>
          T'=\{[A_1]_B=(2,1,0,-2), [A_2]_B=(1,1,1,-1)\}
        </me>
        Translating back to <m>V=M_{22}</m>, we conclude that the corresponding set
        <me>
          B=\{A_1, A_2\}
        </me>
        is a basis for <m>W'=\Span S</m>. We conclude that <m>\dim W'=2 </m>.
      </p>
      <p>
        Lastly the space <m>W</m> of all trace-zero matrices is easily seen to have basis
        <me>
        \left\{
        \begin{amatrix}[rr]1\amp 0\\ 0 \amp -1  \end{amatrix},
        \begin{amatrix}[rr]0\amp 1\\ 0\amp 0  \end{amatrix},
        \begin{amatrix}[rr]0 \amp 0\\ 1\amp 0  \end{amatrix}
        \right\}
        </me>,
        and hence <m>\dim W=3</m>. Since <m>\dim W'\lt\dim W</m>, we conclude that <m>W'\ne W</m>.
      </p>
    </solution>
  </example>



  </subsection>
<xi:include href="./s_coordinatevectors_ex.ptx"/>
</section>
