<exercises xml:id="s_orthogonal_projection_ex">
  <exercisegroup>
    <title></title>
    <introduction>
      In each exercise below you are given an inner product space <m>V</m>, a subspace <m>W=\Span B</m> where <m>B</m> is orthogonal, and a vector <m>\boldv\in V</m>. Compute <m>\proj{\boldv}{W}</m>.
    </introduction>
    <exercise>
      <statement>
        <p>
          <m>V=\R^4</m> with the dot product; <m>W=\Span\{(1,1,1,1),(1,-1,1,-1), (1,1,-1,-1)\}</m>; <m>\boldv=(2,3,-1,1)</m>
        </p>
      </statement>
    </exercise>
    <exercise>
      <statement>
        <p>
          <m>V=\R^3</m> with dot product with weights <m>k_1=1, k_2=2, k_3=1</m>; <m>W=\{(1,1,1), (1,-1,1),(1,0,-1)\}</m>; <m>\boldv=(1,2,3)</m>
        </p>
      </statement>
    </exercise>
    <exercise>
      <statement>
        <p>
          <m>V=C([0,2\pi])</m> with the integral inner product; <m>W=\Span\{\cos x, \cos 2x, \sin x\}</m>; <m>f(x)=3</m> for all <m>x\in [0,2\pi]</m>
        </p>
      </statement>
    </exercise>

  </exercisegroup>
  <exercise>
    <statement>
      <p>
        Let <m>\mathcal{P}\subseteq \R^3</m> be the plane passing through the origin with normal vector <m>\boldn=(1,2,-1)</m>. Find the orthogonal projection of <m>(1,1,1)</m> onto <m>\mathcal{P}</m> with respect to the dot product.
      </p>
    </statement>
  </exercise>

  <exercise>
    <statement>
      <p>
        Recall that the trace <m>\tr A</m> of a square matrix is the sum of its diagonal entries. Let <m>V=M_{22}</m> with inner product <m>\angvec{A,B}=\tr(A^TB)</m>. (You may take for granted that this operation is indeed an inner product on <m>M_{22}</m>.) Define <m>W=\{A\in M_{22}\colon \tr A=0\}</m>.
      </p>
      <ol>
        <li>
          <p>
            Compute an orthogonal basis for <m>W</m>. You can do this either by inspection (the space is manageable), or by starting with any basis of <m>W</m> and applying the Gram-Schmidt procedure.
          </p>
        </li>
        <li>
          <p>
            Compute <m>\proj{A}{W}</m>,
            where
            <me>A=\begin{bmatrix}1\amp 2\\ 1\amp 1 \end{bmatrix}
            </me>.
          </p>
        </li>
      </ol>
    </statement>

  </exercise>

  <exercise>
    <statement>
      <p>
        Let <m>V=C([0,1])</m> with the integral inner product, and let <m>f(x)=x</m>.
        Find the function of the form
        <m>g(x)=a+b\cos(2\pi x)+c\sin(2\pi x)</m> that <q>best approximates</q> <m>f(x)</m> in terms of this inner product: <ie /> find the the <m>g(x)</m> of this form that minimizes <m>d(g,f)</m>.
      </p>
    </statement>
    <hint>
      <p>
        The set <m>S=\{f(x)=1, g(x)=\cos(2\pi x), h(x)=\sin(2\pi x)\}</m> is orthogonal with respect to the given inner product.
      </p>
    </hint>
  </exercise>


  <exercise xml:id="ex_ortho_comp">
    <statement>
      <p>
        Let <m>(V, \langle , \rangle )</m> be an inner product space, let <m>S=\{\boldw_1, \boldw_2, \dots, \boldw_r\}\subseteq V</m>, and let <m>W=\Span S</m>. Prove:
        <me>
        \boldv\in W^\perp \text{ if and only if } \langle \boldv,\boldw_i \rangle=0 \text{ for all } 1\leq i\leq r
        </me>.
        In other words, to check whether an element is in <m>W^\perp</m>, it suffices to check that it is orthogonal to each element of its spanning set <m>S</m>.

      </p>
    </statement>
  </exercise>

  <exercise>
    <statement>
      <p>
        Consider the inner product space <m>\R^4</m> together with the dot product. Let
        <me>
          W=\{(x_1,x_2,x_3,x_4)\in \R^4\colon x_1=x_3 \text{ and } x_2=x_4\}.
        </me>
        Provide orthogonal bases for <m>W</m> and <m>W^\perp</m>.
      </p>
    </statement>
  </exercise>

  <exercise xml:id="ex_orthocomp_subspace">
    <statement>
      <p>
        Prove statements (1) and (2) of <xref ref="th_orthogonal_complement"/>.
      </p>
    </statement>
  </exercise>
  <exercise xml:id="ex_orthocomp_dim">
    <title>Dimension of <m>W^\perp</m></title>
    <statement>
      <p>
        Prove statement (3) of <xref ref="th_orthogonal_complement"/>:  if <m>(V, \ \angvec{\ , \ })</m> is an inner product space of dimension <m>n</m>, and <m>W</m> is a subspace of <m>V</m>, then
        <me>
          \dim W+\dim W^\perp=n
        </me>.
      </p>
    </statement>
    <hint>
      <p>
        By <xref ref="cor_orthonormal_existence"/> there is an orthogonal basis
        <m>B=\{\boldv_1,\dots ,\boldv_r\}</m> of <m>W</m>, and furthermore, we can extend <m>B</m> to an orthogonal
        basis <m>B'=\{\boldv_1,\boldv_2, \dots, \boldv_r, \boldu_1,\dots , \boldu_{n-r}\}</m> of all of <m>V</m>.
        Show the <m>\boldu_i</m> form a basis for <m>W^\perp</m>.
      </p>
    </hint>
  </exercise>
  <exercise>
    <statement>
      <p>
        Prove <xref ref="cor_orthoproj_linear"/> following the suggestion in the text.
      </p>
    </statement>

  </exercise>
  <exercise xml:id="ex_orthoproj_props">
    <statement>
      <p>
        Let <m>V</m> an inner product space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
        Prove the following statements:
        </p>
        <ol>
          <li>
            <p>
             <m>\boldv\in W</m> if and only if <m>\proj{\boldv}{W}=\boldv</m>;
            </p>
          </li>
          <li>
            <p>
              <m>\boldv\in W^\perp</m> if and only if  <m>\proj{\boldv}{W}=\boldzero</m>.
            </p>
          </li>
        </ol>

    </statement>
  </exercise>





  <exercise>
    <statement>
      <p>
        We consider the problem of fitting a collection of data points <m>(x,y)</m> with a quadratic curve of the form <m>y=f(x)=ax^2+bx+c</m>.
        Thus we are <em>given</em> some collection of points <m>(x,y)</m>,
        and we <em>seek</em> parameters
        <m>a,
        b, c</m> for which the graph of <m>f(x)=ax^2+bx+c</m>
        <q>best fits</q>
        the points in some way.
      </p>
      <ol>
        <li>
          <p>
            Show, using linear algebra,
            that if we are given any three points <m>(x,y)=(r_1,s_1), (r_2,s_2), (r_3,s_3)</m>,
            where the <m>x</m>-coordinates <m>r_i</m> are all distinct,
            then there is a <em>unique</em>
            choice of <m>a,b,c</m> such that the corresponding quadratic function agrees
            <em>precisely</em> with the data.
            In other words, given just about any three points in the plane,
            there is a unique quadratic curve connecting them.
          </p>
        </li>
        <li>
          <p>
            Now suppose we are given the four data points
            <me>
            P_1=(0,2), P_2=(1,0), P_3=(2,2), P_4=(3,6)
            </me>.
          </p>
          <ol>
            <li>
              <p>
                Use the least-squares method described in the lecture notes to come up with a quadratic function <m>y=f(x)</m> that
                <q>best fits</q>
                the data.
              </p>
            </li>
            <li>
              <p>
                Graph the function <m>f</m> you found,
                along with the points <m>P_i</m>.
                (You may want to use technology.)
                Use your graph to explain precisely in what sense <m>f</m>
                <q>best fits</q>
                the data.
              </p>
            </li>
          </ol>
        </li>
      </ol>
    </statement>
  </exercise>
</exercises>
